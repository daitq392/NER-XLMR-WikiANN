{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7349024,"sourceType":"datasetVersion","datasetId":4257619}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-06T08:00:44.007380Z","iopub.execute_input":"2024-01-06T08:00:44.007854Z","iopub.status.idle":"2024-01-06T08:00:44.503277Z","shell.execute_reply.started":"2024-01-06T08:00:44.007813Z","shell.execute_reply":"2024-01-06T08:00:44.501995Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/xtremepanx/test.nl\n/kaggle/input/xtremepanx/test.vi\n/kaggle/input/xtremepanx/test.fr\n/kaggle/input/xtremepanx/dev.zh\n/kaggle/input/xtremepanx/test.zh\n/kaggle/input/xtremepanx/dev.nl\n/kaggle/input/xtremepanx/train.zh\n/kaggle/input/xtremepanx/train.vi\n/kaggle/input/xtremepanx/dev.en\n/kaggle/input/xtremepanx/dev.vi\n/kaggle/input/xtremepanx/dev.fr\n/kaggle/input/xtremepanx/train.fr\n/kaggle/input/xtremepanx/test.en\n/kaggle/input/xtremepanx/train.nl\n/kaggle/input/xtremepanx/train.en\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/input/xtremepanx","metadata":{"execution":{"iopub.status.busy":"2024-01-06T08:00:44.505382Z","iopub.execute_input":"2024-01-06T08:00:44.505899Z","iopub.status.idle":"2024-01-06T08:00:44.513065Z","shell.execute_reply.started":"2024-01-06T08:00:44.505856Z","shell.execute_reply":"2024-01-06T08:00:44.511925Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/xtremepanx\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import Dataset, ClassLabel, Value, Features, Sequence\n\ndef convert_to_hf_dataset(input_file, class_names):\n    with open(input_file, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n\n    sentences = []\n    current_tokens = []\n    current_tags = []\n\n    for line in lines:\n        line = line.strip()\n        if line:\n            language, token_tag = line.split(':', 1)\n            token, tag = token_tag.split('\\t')\n            current_tokens.append(token)\n            current_tags.append(tag)\n        else:  # New sentence\n            sentences.append({'tokens': current_tokens, 'ner_tags': current_tags})\n            current_tokens = []\n            current_tags = []\n\n    # Handle the last sentence if there is one\n    if current_tokens:\n        sentences.append({'tokens': current_tokens, 'ner_tags': current_tags})\n\n    # Define the ClassLabel feature for ner_tags\n    class_labels = ClassLabel(names=class_names)\n\n    # Define the Features for the Hugging Face dataset\n    features = Features({\n        'tokens': Sequence(Value('string')),\n        'ner_tags': Sequence(class_labels)\n    })\n\n    # Convert the list of dictionaries to a dictionary of lists\n    hf_dataset = {key: [entry[key] for entry in sentences] for key in sentences[0]}\n\n    return Dataset.from_dict(hf_dataset, features=features)\n\n# Example usage for three input files\nclass_names = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n#dataset_3 = convert_to_hf_dataset('test.vi', class_names)\n\n# Save the datasets to disk\n#dataset_3.save_to_disk('/kaggle/working/output_dataset_2')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-06T08:00:44.515139Z","iopub.execute_input":"2024-01-06T08:00:44.515511Z","iopub.status.idle":"2024-01-06T08:00:46.303620Z","shell.execute_reply.started":"2024-01-06T08:00:44.515478Z","shell.execute_reply":"2024-01-06T08:00:46.302627Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\n\n# Assuming you have already converted the individual files to Hugging Face datasets\nclass_names = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n\ntrain_dataset = convert_to_hf_dataset('train.vi', class_names)\ndev_dataset = convert_to_hf_dataset('dev.vi', class_names)\ntest_dataset = convert_to_hf_dataset('test.vi', class_names)\n\n# Create a DatasetDict\ndataset_dict = DatasetDict({\n    'train': train_dataset,\n    'validation': dev_dataset,\n    'test': test_dataset,\n})\n","metadata":{"execution":{"iopub.status.busy":"2024-01-06T08:00:46.306868Z","iopub.execute_input":"2024-01-06T08:00:46.308222Z","iopub.status.idle":"2024-01-06T08:00:49.380499Z","shell.execute_reply.started":"2024-01-06T08:00:46.308166Z","shell.execute_reply":"2024-01-06T08:00:49.379128Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dataset_dict","metadata":{"execution":{"iopub.status.busy":"2024-01-06T08:00:49.381942Z","iopub.execute_input":"2024-01-06T08:00:49.382276Z","iopub.status.idle":"2024-01-06T08:00:49.392555Z","shell.execute_reply.started":"2024-01-06T08:00:49.382247Z","shell.execute_reply":"2024-01-06T08:00:49.391393Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['tokens', 'ner_tags'],\n        num_rows: 20000\n    })\n    validation: Dataset({\n        features: ['tokens', 'ner_tags'],\n        num_rows: 10000\n    })\n    test: Dataset({\n        features: ['tokens', 'ner_tags'],\n        num_rows: 10000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"%cd /kaggle/input/xtremepanx","metadata":{"execution":{"iopub.status.busy":"2024-01-06T08:00:49.394067Z","iopub.execute_input":"2024-01-06T08:00:49.394513Z","iopub.status.idle":"2024-01-06T08:00:49.406831Z","shell.execute_reply.started":"2024-01-06T08:00:49.394479Z","shell.execute_reply":"2024-01-06T08:00:49.405930Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/input/xtremepanx\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import DatasetDict\nfrom collections import defaultdict\n\n# Assuming you have a list of language codes\nlanguages = ['en', 'vi', 'fr', 'nl', 'zh']  # Add more languages as needed\n\n# Initialize a defaultdict of DatasetDict\nall_datasets = defaultdict(DatasetDict)\n\n# Iterate over each language\nfor lang in languages:\n    train_file = f'train.{lang}'\n    dev_file = f'dev.{lang}'\n    test_file = f'test.{lang}'\n\n    # Convert individual language files to Hugging Face datasets\n    train_dataset = convert_to_hf_dataset(train_file, class_names)\n    dev_dataset = convert_to_hf_dataset(dev_file, class_names)\n    test_dataset = convert_to_hf_dataset(test_file, class_names)\n\n    # Create a DatasetDict for the language\n    lang_dataset_dict = DatasetDict({\n        'train': train_dataset,\n        'validation': dev_dataset,\n        'test': test_dataset,\n    })\n\n    # Add the language DatasetDict to the overall defaultdict\n    all_datasets[lang] = lang_dataset_dict\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-06T08:00:49.408308Z","iopub.execute_input":"2024-01-06T08:00:49.409669Z","iopub.status.idle":"2024-01-06T08:01:10.242904Z","shell.execute_reply.started":"2024-01-06T08:00:49.409412Z","shell.execute_reply":"2024-01-06T08:01:10.241600Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"all_datasets","metadata":{"execution":{"iopub.status.busy":"2024-01-06T08:01:10.244384Z","iopub.execute_input":"2024-01-06T08:01:10.244759Z","iopub.status.idle":"2024-01-06T08:01:10.253354Z","shell.execute_reply.started":"2024-01-06T08:01:10.244729Z","shell.execute_reply":"2024-01-06T08:01:10.252007Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"defaultdict(datasets.dataset_dict.DatasetDict,\n            {'en': DatasetDict({\n                 train: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 20000\n                 })\n                 validation: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n                 test: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n             }),\n             'vi': DatasetDict({\n                 train: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 20000\n                 })\n                 validation: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n                 test: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n             }),\n             'fr': DatasetDict({\n                 train: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 20000\n                 })\n                 validation: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n                 test: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n             }),\n             'nl': DatasetDict({\n                 train: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 20000\n                 })\n                 validation: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n                 test: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n             }),\n             'zh': DatasetDict({\n                 train: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 20000\n                 })\n                 validation: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n                 test: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n             })})"},"metadata":{}}]},{"cell_type":"code","source":"# Save the defaultdict of DatasetDict\nfor lang, lang_dataset_dict in all_datasets.items():\n    lang_dataset_dict.save_to_disk(f'/kaggle/working/save/{lang}_datasets')","metadata":{"execution":{"iopub.status.busy":"2024-01-06T08:01:10.254907Z","iopub.execute_input":"2024-01-06T08:01:10.255350Z","iopub.status.idle":"2024-01-06T08:01:10.339430Z","shell.execute_reply.started":"2024-01-06T08:01:10.255315Z","shell.execute_reply":"2024-01-06T08:01:10.338229Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!zip -r /kaggle/working/save.zip /kaggle/working/save","metadata":{"execution":{"iopub.status.busy":"2024-01-06T08:01:10.342867Z","iopub.execute_input":"2024-01-06T08:01:10.343284Z","iopub.status.idle":"2024-01-06T08:01:13.773230Z","shell.execute_reply.started":"2024-01-06T08:01:10.343250Z","shell.execute_reply":"2024-01-06T08:01:13.772104Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"  adding: kaggle/working/save/ (stored 0%)\n  adding: kaggle/working/save/vi_datasets/ (stored 0%)\n  adding: kaggle/working/save/vi_datasets/dataset_dict.json (deflated 5%)\n  adding: kaggle/working/save/vi_datasets/validation/ (stored 0%)\n  adding: kaggle/working/save/vi_datasets/validation/dataset.arrow (deflated 75%)\n  adding: kaggle/working/save/vi_datasets/validation/dataset_info.json (deflated 65%)\n  adding: kaggle/working/save/vi_datasets/validation/state.json (deflated 40%)\n  adding: kaggle/working/save/vi_datasets/train/ (stored 0%)\n  adding: kaggle/working/save/vi_datasets/train/dataset.arrow (deflated 75%)\n  adding: kaggle/working/save/vi_datasets/train/dataset_info.json (deflated 65%)\n  adding: kaggle/working/save/vi_datasets/train/state.json (deflated 40%)\n  adding: kaggle/working/save/vi_datasets/test/ (stored 0%)\n  adding: kaggle/working/save/vi_datasets/test/dataset.arrow (deflated 75%)\n  adding: kaggle/working/save/vi_datasets/test/dataset_info.json (deflated 65%)\n  adding: kaggle/working/save/vi_datasets/test/state.json (deflated 40%)\n  adding: kaggle/working/save/nl_datasets/ (stored 0%)\n  adding: kaggle/working/save/nl_datasets/dataset_dict.json (deflated 5%)\n  adding: kaggle/working/save/nl_datasets/validation/ (stored 0%)\n  adding: kaggle/working/save/nl_datasets/validation/dataset.arrow (deflated 75%)\n  adding: kaggle/working/save/nl_datasets/validation/dataset_info.json (deflated 65%)\n  adding: kaggle/working/save/nl_datasets/validation/state.json (deflated 40%)\n  adding: kaggle/working/save/nl_datasets/train/ (stored 0%)\n  adding: kaggle/working/save/nl_datasets/train/dataset.arrow (deflated 75%)\n  adding: kaggle/working/save/nl_datasets/train/dataset_info.json (deflated 65%)\n  adding: kaggle/working/save/nl_datasets/train/state.json (deflated 40%)\n  adding: kaggle/working/save/nl_datasets/test/ (stored 0%)\n  adding: kaggle/working/save/nl_datasets/test/dataset.arrow (deflated 75%)\n  adding: kaggle/working/save/nl_datasets/test/dataset_info.json (deflated 65%)\n  adding: kaggle/working/save/nl_datasets/test/state.json (deflated 41%)\n  adding: kaggle/working/save/zh_datasets/ (stored 0%)\n  adding: kaggle/working/save/zh_datasets/dataset_dict.json (deflated 5%)\n  adding: kaggle/working/save/zh_datasets/validation/ (stored 0%)\n  adding: kaggle/working/save/zh_datasets/validation/dataset.arrow (deflated 79%)\n  adding: kaggle/working/save/zh_datasets/validation/dataset_info.json (deflated 65%)\n  adding: kaggle/working/save/zh_datasets/validation/state.json (deflated 40%)\n  adding: kaggle/working/save/zh_datasets/train/ (stored 0%)\n  adding: kaggle/working/save/zh_datasets/train/dataset.arrow (deflated 79%)\n  adding: kaggle/working/save/zh_datasets/train/dataset_info.json (deflated 65%)\n  adding: kaggle/working/save/zh_datasets/train/state.json (deflated 41%)\n  adding: kaggle/working/save/zh_datasets/test/ (stored 0%)\n  adding: kaggle/working/save/zh_datasets/test/dataset.arrow (deflated 79%)\n  adding: kaggle/working/save/zh_datasets/test/dataset_info.json (deflated 65%)\n  adding: kaggle/working/save/zh_datasets/test/state.json (deflated 40%)\n  adding: kaggle/working/save/en_datasets/ (stored 0%)\n  adding: kaggle/working/save/en_datasets/dataset_dict.json (deflated 5%)\n  adding: kaggle/working/save/en_datasets/validation/ (stored 0%)\n  adding: kaggle/working/save/en_datasets/validation/dataset.arrow (deflated 74%)\n  adding: kaggle/working/save/en_datasets/validation/dataset_info.json (deflated 65%)\n  adding: kaggle/working/save/en_datasets/validation/state.json (deflated 41%)\n  adding: kaggle/working/save/en_datasets/train/ (stored 0%)\n  adding: kaggle/working/save/en_datasets/train/dataset.arrow (deflated 74%)\n  adding: kaggle/working/save/en_datasets/train/dataset_info.json (deflated 65%)\n  adding: kaggle/working/save/en_datasets/train/state.json (deflated 41%)\n  adding: kaggle/working/save/en_datasets/test/ (stored 0%)\n  adding: kaggle/working/save/en_datasets/test/dataset.arrow (deflated 74%)\n  adding: kaggle/working/save/en_datasets/test/dataset_info.json (deflated 65%)\n  adding: kaggle/working/save/en_datasets/test/state.json (deflated 41%)\n  adding: kaggle/working/save/fr_datasets/ (stored 0%)\n  adding: kaggle/working/save/fr_datasets/dataset_dict.json (deflated 5%)\n  adding: kaggle/working/save/fr_datasets/validation/ (stored 0%)\n  adding: kaggle/working/save/fr_datasets/validation/dataset.arrow (deflated 74%)\n  adding: kaggle/working/save/fr_datasets/validation/dataset_info.json (deflated 65%)\n  adding: kaggle/working/save/fr_datasets/validation/state.json (deflated 40%)\n  adding: kaggle/working/save/fr_datasets/train/ (stored 0%)\n  adding: kaggle/working/save/fr_datasets/train/dataset.arrow (deflated 74%)\n  adding: kaggle/working/save/fr_datasets/train/dataset_info.json (deflated 65%)\n  adding: kaggle/working/save/fr_datasets/train/state.json (deflated 40%)\n  adding: kaggle/working/save/fr_datasets/test/ (stored 0%)\n  adding: kaggle/working/save/fr_datasets/test/dataset.arrow (deflated 74%)\n  adding: kaggle/working/save/fr_datasets/test/dataset_info.json (deflated 65%)\n  adding: kaggle/working/save/fr_datasets/test/state.json (deflated 41%)\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/save","metadata":{"execution":{"iopub.status.busy":"2024-01-16T10:21:03.519314Z","iopub.execute_input":"2024-01-16T10:21:03.519664Z","iopub.status.idle":"2024-01-16T10:21:03.525375Z","shell.execute_reply.started":"2024-01-16T10:21:03.519636Z","shell.execute_reply":"2024-01-16T10:21:03.524254Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/working/save\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import DatasetDict\nfrom collections import defaultdict\n","metadata":{"execution":{"iopub.status.busy":"2024-01-16T10:21:03.526936Z","iopub.execute_input":"2024-01-16T10:21:03.527848Z","iopub.status.idle":"2024-01-16T10:21:04.991325Z","shell.execute_reply.started":"2024-01-16T10:21:03.527819Z","shell.execute_reply":"2024-01-16T10:21:04.990446Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"languages = ['en', 'vi', 'fr', 'nl', 'zh']","metadata":{"execution":{"iopub.status.busy":"2024-01-16T10:21:04.993637Z","iopub.execute_input":"2024-01-16T10:21:04.994126Z","iopub.status.idle":"2024-01-16T10:21:04.998386Z","shell.execute_reply.started":"2024-01-16T10:21:04.994100Z","shell.execute_reply":"2024-01-16T10:21:04.997218Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"loaded_datasets2 = defaultdict(DatasetDict)\nfor lang in languages:\n    loaded_datasets2[lang] = DatasetDict.load_from_disk(f'{lang}_datasets')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-16T10:36:08.398797Z","iopub.execute_input":"2024-01-16T10:36:08.399232Z","iopub.status.idle":"2024-01-16T10:36:08.571595Z","shell.execute_reply.started":"2024-01-16T10:36:08.399202Z","shell.execute_reply":"2024-01-16T10:36:08.570549Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"loaded_datasets2","metadata":{"execution":{"iopub.status.busy":"2024-01-16T10:36:11.416335Z","iopub.execute_input":"2024-01-16T10:36:11.417676Z","iopub.status.idle":"2024-01-16T10:36:11.427783Z","shell.execute_reply.started":"2024-01-16T10:36:11.417604Z","shell.execute_reply":"2024-01-16T10:36:11.426130Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"defaultdict(datasets.dataset_dict.DatasetDict,\n            {'en': DatasetDict({\n                 train: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 20000\n                 })\n                 validation: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n                 test: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n             }),\n             'vi': DatasetDict({\n                 train: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 20000\n                 })\n                 validation: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n                 test: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n             }),\n             'fr': DatasetDict({\n                 train: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 20000\n                 })\n                 validation: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n                 test: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n             }),\n             'nl': DatasetDict({\n                 train: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 20000\n                 })\n                 validation: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n                 test: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n             }),\n             'zh': DatasetDict({\n                 train: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 20000\n                 })\n                 validation: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n                 test: Dataset({\n                     features: ['tokens', 'ner_tags'],\n                     num_rows: 10000\n                 })\n             })})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Examine tags distribution ","metadata":{}},{"cell_type":"code","source":"for lang in languages:\n    tags = loaded_datasets2[lang][\"train\"].features[\"ner_tags\"].feature\n    \n    def create_tag_names(batch):\n        return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n    \n    # Map the create_tag_names function to each split in the language\n    for split in loaded_datasets2[lang].keys():\n        loaded_datasets2[lang][split] = loaded_datasets2[lang][split].map(create_tag_names)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-16T10:37:26.299860Z","iopub.execute_input":"2024-01-16T10:37:26.301297Z","iopub.status.idle":"2024-01-16T10:37:45.985423Z","shell.execute_reply.started":"2024-01-16T10:37:26.301251Z","shell.execute_reply":"2024-01-16T10:37:45.984202Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12772947a5c641f79ce6524ead99c7a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e86b42bb941b4156ab9d1f8f332a63fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e2db9e94602423e9507b1b3ad6bd73d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60e7ea28f93148ada79289c66999be35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31673f4ebcf04daea60199e004c4f9f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e521e0f8ce84f63bbe1bd80867bbc0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"041997ac106b4904ad487466f9d2f164"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc4859760b9943f1b0bb630e619defe4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"652f568cfa99420d91f657246fe9e4c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62207a5de69c4015897fe064cdeb1348"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"906d167d65c844b28737b28353d48858"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe6e24879b5841c2aacb1ccf1460d1ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab2b1344489d4841a804bf0ff451d68e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0f994ff2fd544dc810582191c75080c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10000 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85106b5c5b374e57972890675246d191"}},"metadata":{}}]},{"cell_type":"code","source":"split2freqs = defaultdict(Counter)\n\n# Iterate over each language and split\nfor lang in languages:\n    for split, dataset in loaded_datasets2[lang].items():\n        for row in dataset[\"ner_tags_str\"]:\n            for tag in row:\n                if tag.startswith(\"B\"):\n                    tag_type = tag.split(\"-\")[1]\n                    split2freqs[(lang, split)][tag_type] += 1\n\n# Convert the result to a DataFrame\nresult_df = pd.DataFrame.from_dict(split2freqs, orient=\"columns\").fillna(0)\n\n# Display the result\nprint(result_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-16T10:57:32.813154Z","iopub.execute_input":"2024-01-16T10:57:32.813616Z","iopub.status.idle":"2024-01-16T10:57:35.448522Z","shell.execute_reply.started":"2024-01-16T10:57:32.813566Z","shell.execute_reply":"2024-01-16T10:57:35.447602Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"       en                     vi                     fr                   \\\n    train validation  test train validation  test train validation  test   \nORG  9422       4677  4745  7364       3621  3704  7764       3910  3885   \nPER  9164       4635  4556  7470       3738  3884  8965       4406  4499   \nLOC  9345       4834  4657  7588       3832  3717  9718       4840  4985   \n\n       nl                     zh                   \n    train validation  test train validation  test  \nORG  7778       3943  3908  7684       3950  3779  \nPER  9308       4754  4684  7879       3845  3899  \nLOC  9964       4835  5133  8572       4222  4371  \n","output_type":"stream"}]}]}